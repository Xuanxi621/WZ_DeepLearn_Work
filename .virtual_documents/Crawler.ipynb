import os
import time
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm


# ===============================
# ⚙️ 参数配置
# ===============================
birds = [
    "Black-footed Albatross",
    "Laysan Albatross",
    "Sooty_Albatross",
    "Groove_billed_Ani",
    "Crested_Auklet",
    "Least_Auklet",
    "Parakeet_Auklet",
    "Rhinoceros_Auklet",
    "Brewer_Blackbird",
    "Red_winged_Blackbird"
]

OUTPUT_DIR = "dataset"
LIMIT_PER_BIRD = 600
PER_PAGE = 50
SLEEP_TIME = 1.0
MAX_THREADS = 8     # 并行下载线程数


# ===============================
# 🌐 爬取 Bing 图片 URL（翻页）
# ===============================
def fetch_image_urls(query, limit=600, per_page=50, session=None, sleep_time=1.0):
    if session is None:
        session = requests.Session()

    headers = {
        "User-Agent": (
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/114.0.0.0 Safari/537.36"
        )
    }

    urls = []
    offset = 0
    page = 1

    while len(urls) < limit:
        print(f"📄 [{query}] 第 {page} 页 (offset={offset})")
        url = f"https://www.bing.com/images/search?q={query}&form=HDRSC2&first={offset}"
        resp = session.get(url, headers=headers, timeout=10)

        if resp.status_code != 200:
            print(f"⚠️ 第 {page} 页请求失败: 状态码 {resp.status_code}")
            break

        soup = BeautifulSoup(resp.text, "html.parser")
        img_elements = soup.find_all("img")

        new_urls = []
        for img in img_elements:
            src = img.get("src") or img.get("data-src")
            if src and src.startswith("https://") and src not in urls:
                new_urls.append(src)

        if not new_urls:
            print("❌ 没有更多图片，提前结束。")
            break

        urls.extend(new_urls)
        print(f"✅ 第 {page} 页新增 {len(new_urls)} 张，累计 {len(urls)} 张。")

        offset += per_page
        page += 1
        time.sleep(sleep_time)

    print(f"📸 [{query}] 共抓取 {len(urls)} 张（目标 {limit}）\n")
    return urls[:limit]


# ===============================
# 💾 并行下载图片（带进度条）
# ===============================
def download_single_image(url, folder, idx):
    try:
        resp = requests.get(url, timeout=10)
        resp.raise_for_status()
        ext = os.path.splitext(urlparse(url).path)[1]
        if ext.lower() not in [".jpg", ".jpeg", ".png"]:
            ext = ".jpg"
        fname = os.path.join(folder, f"{idx:04d}{ext}")
        with open(fname, "wb") as f:
            f.write(resp.content)
        return True
    except Exception:
        return False


def download_images_parallel(urls, folder, max_threads=8):
    os.makedirs(folder, exist_ok=True)
    success = 0

    with ThreadPoolExecutor(max_workers=max_threads) as executor:
        futures = [
            executor.submit(download_single_image, url, folder, i)
            for i, url in enumerate(urls, start=1)
        ]
        for _ in tqdm(as_completed(futures), total=len(futures), desc=f"📥 {os.path.basename(folder)}", unit="img"):
            success += 1

    print(f"✅ {os.path.basename(folder)} 下载完成，共 {success} 张图片。")


# ===============================
# 🚀 主程序：并行处理多个类别
# ===============================
start_time = time.time()
print("🚀 开始批量爬取鸟类图片...\n")

for bird in birds:
    folder = os.path.join(OUTPUT_DIR, bird.replace(" ", "_"))
    print(f"==============================")
    print(f"🦉 开始爬取：{bird}")
    print(f"==============================")
    urls = fetch_image_urls(bird, limit=LIMIT_PER_BIRD, per_page=PER_PAGE, sleep_time=SLEEP_TIME)
    download_images_parallel(urls, folder, max_threads=MAX_THREADS)

print(f"\n🎉 所有鸟类下载完成！总耗时 {time.time() - start_time:.2f} 秒")



