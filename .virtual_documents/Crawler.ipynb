import os
import time
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm


# ===============================
# âš™ï¸ å‚æ•°é…ç½®
# ===============================
birds = [
    "Black-footed Albatross",
    "Laysan Albatross",
    "Sooty_Albatross",
    "Groove_billed_Ani",
    "Crested_Auklet",
    "Least_Auklet",
    "Parakeet_Auklet",
    "Rhinoceros_Auklet",
    "Brewer_Blackbird",
    "Red_winged_Blackbird"
]

OUTPUT_DIR = "dataset"
LIMIT_PER_BIRD = 600
PER_PAGE = 50
SLEEP_TIME = 1.0
MAX_THREADS = 8     # å¹¶è¡Œä¸‹è½½çº¿ç¨‹æ•°


# ===============================
# ğŸŒ çˆ¬å– Bing å›¾ç‰‡ URLï¼ˆç¿»é¡µï¼‰
# ===============================
def fetch_image_urls(query, limit=600, per_page=50, session=None, sleep_time=1.0):
    if session is None:
        session = requests.Session()

    headers = {
        "User-Agent": (
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/114.0.0.0 Safari/537.36"
        )
    }

    urls = []
    offset = 0
    page = 1

    while len(urls) < limit:
        print(f"ğŸ“„ [{query}] ç¬¬ {page} é¡µ (offset={offset})")
        url = f"https://www.bing.com/images/search?q={query}&form=HDRSC2&first={offset}"
        resp = session.get(url, headers=headers, timeout=10)

        if resp.status_code != 200:
            print(f"âš ï¸ ç¬¬ {page} é¡µè¯·æ±‚å¤±è´¥: çŠ¶æ€ç  {resp.status_code}")
            break

        soup = BeautifulSoup(resp.text, "html.parser")
        img_elements = soup.find_all("img")

        new_urls = []
        for img in img_elements:
            src = img.get("src") or img.get("data-src")
            if src and src.startswith("https://") and src not in urls:
                new_urls.append(src)

        if not new_urls:
            print("âŒ æ²¡æœ‰æ›´å¤šå›¾ç‰‡ï¼Œæå‰ç»“æŸã€‚")
            break

        urls.extend(new_urls)
        print(f"âœ… ç¬¬ {page} é¡µæ–°å¢ {len(new_urls)} å¼ ï¼Œç´¯è®¡ {len(urls)} å¼ ã€‚")

        offset += per_page
        page += 1
        time.sleep(sleep_time)

    print(f"ğŸ“¸ [{query}] å…±æŠ“å– {len(urls)} å¼ ï¼ˆç›®æ ‡ {limit}ï¼‰\n")
    return urls[:limit]


# ===============================
# ğŸ’¾ å¹¶è¡Œä¸‹è½½å›¾ç‰‡ï¼ˆå¸¦è¿›åº¦æ¡ï¼‰
# ===============================
def download_single_image(url, folder, idx):
    try:
        resp = requests.get(url, timeout=10)
        resp.raise_for_status()
        ext = os.path.splitext(urlparse(url).path)[1]
        if ext.lower() not in [".jpg", ".jpeg", ".png"]:
            ext = ".jpg"
        fname = os.path.join(folder, f"{idx:04d}{ext}")
        with open(fname, "wb") as f:
            f.write(resp.content)
        return True
    except Exception:
        return False


def download_images_parallel(urls, folder, max_threads=8):
    os.makedirs(folder, exist_ok=True)
    success = 0

    with ThreadPoolExecutor(max_workers=max_threads) as executor:
        futures = [
            executor.submit(download_single_image, url, folder, i)
            for i, url in enumerate(urls, start=1)
        ]
        for _ in tqdm(as_completed(futures), total=len(futures), desc=f"ğŸ“¥ {os.path.basename(folder)}", unit="img"):
            success += 1

    print(f"âœ… {os.path.basename(folder)} ä¸‹è½½å®Œæˆï¼Œå…± {success} å¼ å›¾ç‰‡ã€‚")


# ===============================
# ğŸš€ ä¸»ç¨‹åºï¼šå¹¶è¡Œå¤„ç†å¤šä¸ªç±»åˆ«
# ===============================
start_time = time.time()
print("ğŸš€ å¼€å§‹æ‰¹é‡çˆ¬å–é¸Ÿç±»å›¾ç‰‡...\n")

for bird in birds:
    folder = os.path.join(OUTPUT_DIR, bird.replace(" ", "_"))
    print(f"==============================")
    print(f"ğŸ¦‰ å¼€å§‹çˆ¬å–ï¼š{bird}")
    print(f"==============================")
    urls = fetch_image_urls(bird, limit=LIMIT_PER_BIRD, per_page=PER_PAGE, sleep_time=SLEEP_TIME)
    download_images_parallel(urls, folder, max_threads=MAX_THREADS)

print(f"\nğŸ‰ æ‰€æœ‰é¸Ÿç±»ä¸‹è½½å®Œæˆï¼æ€»è€—æ—¶ {time.time() - start_time:.2f} ç§’")



